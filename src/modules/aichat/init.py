from loguru import logger
from transformers import AutoTokenizer, AutoModelForCausalLM


def generate_chat(input_text):
    try:
        logger.info("Loading pytorch")
        from torch import torch
        logger.success("Loading pytorch done")
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
    except:
        device = "cpu"
        logger.error("Pytorch not found, defaulting to CPU")

    # TODO have to clear up the logging a bit and make it more readable. This shit sucks haha
    logger.info("Checking if CUDA is available")
    logger.info(f"Using device: {device}")

    logger.info("Loading tokenizer and model")
    tokenizer = AutoTokenizer.from_pretrained("PygmalionAI/pygmalion-350m")
    model = AutoModelForCausalLM.from_pretrained("PygmalionAI/pygmalion-350m").to(device)
    logger.success("Tokenizer and model finished")

    logger.info("Generating output")
    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)
    output = model.generate(input_ids, max_length=50, do_sample=True)
    logger.success("Generation finished")

    logger.info("Decoding output")
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
    logger.success("Decoding finished")

    logger.info(f"This is the text generated by the model: {generated_text}")
    return generated_text
